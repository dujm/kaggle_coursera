\documentclass[11pt, twoside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
%% packages

\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
\usepackage{fullpage}								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\renewcommand{\baselinestretch}{1.5} 
\usepackage{url}
\usepackage{graphicx}%% image 

\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}   % forcing linebreaks in \url
\usepackage
[
        a4paper,% other options: a3paper, a5paper, etc
        left=2.5cm,
        right=2.5cm,
        top=2cm,
        bottom=3cm,
        % use vmargin=2cm to make vertical margins equal to 2cm.
        % us  hmargin=3cm to make horizontal margins equal to 3cm.
        % use margin=3cm to make all margins  equal to 3cm.
]
{geometry}


%\textbf{randome} Some math: $2+2=5$
%%\usepackage{geometry} [a4paper, total={6in, 8in}]               		% See geometry.pdf to learn the layout options. There are lots.		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Coursera: How to Win a Data Science Competition: Learn from Top Kagglers}
\author{DJ's note}
\date{2018}% Activate to display a given date or no date							
\begin{document}%preamble
\maketitle %%title
\tableofcontents %% Content 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Week 1}
\subsection{Recap of main ML algorithms}
\subsubsection{Random Forests}
\paragraph{}
\textbf{Random forest is a collection of many decision trees. }
%This 3D surface plot makes it far more accessible.
%Random Forest models are a "forest" of decision trees. 
Each decision tree is trained on a subset of variables and observations from your dataset.

Each tree cuts the data based on your predictors into groups that are as similar as possible based on your target variable. As each tree is trained on different variables and data, the trees can vary considerably.

So, when we want to predict our target given our inputs, we ask our decision trees and take the average prediction (or majority vote for classification) given those values.

Are you seeing the forest from the trees?
When is a RF a poor choice relative to other algorithms? 
What does a Random Forest model look like? What's happening inside the black box? 

%Would you like to run it yourself? Here's my code

%\url{https://lnkd.in/ggu4iTd}, \textup{animation} \url{https://www.linkedin.com/feed/update/urn:li:activity:6480022057856966656}

\subsubsection{H2O}
\textit{H20} is the open-source \textit{in-memory} prediction engine for Big Data Science.  
\\\url{http://manishbarnwal.com/blog/2017/03/28/h2o_with_r/}
\\\url{https://github.com/h2oai/h2o-3}

\subsection{Overview of methods}
\textup{}Scikit-Learn (or sklearn) library
\\Overview of k-NN (sklearn's documentation)
\\Overview of Linear Models (sklearn's documentation)
\\Overview of Decision Trees (sklearn's documentation)
\\Overview of algorithms and parameters in H2O documentation
\\Additional Tools
\\Vowpal Wabbit repository
\\XGBoost repository
\\LightGBM repository
\\Interactive demo of simple feed-forward Neural Net
\\Frameworks for Neural Nets: Keras,PyTorch,TensorFlow,MXNet, Lasagne
\\Example from sklearn with different decision surfaces
\\Arbitrary order factorization machines

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Software/Hardware requirements}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feature processing and generation with respect to models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Different features}
\begin{itemize}
  \item Numeric
  \item Ordinal: ordered categorical feature (ticket classes, driver license, and education
  \item Text
  \item ID 
  \item Categorical
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Feature processing and generation pipelines depend on a model type}
\textup{e.g. One Hot encoders is useful, however, Random Forrest does not need this}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
\subsubsection{Numeric feature preprocessing}
\textbf{Outliers}
\\\textbf{Rank}
\\\indent If no time to handle outliers in linear and NN models 
\\\indent scipy.stats.rankdata

\textbf {Note: to apply rank to the test data, you need to}
\begin{itemize}
  \item Store the creative mapping from feature values to their rank
  \item Cacatenate train and test data before applying rank transformation 
\end{itemize}

\noindent \textbf{Numeric feature}
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
  \item Numeric feature processing is different for tree and non-tree models
    \begin{enumerate}
      \item Tree-based models don't depend on scaling
      \item Non-tree based models hugely depend on scaling
    \end{enumerate}
    
      \item Most often used preprocessings are
        \begin{enumerate}
           \item MinMaxScaler [0, 1]
           \item StandardScaler to mean ==0, std==1 
           \item Rank sets spaces between sorted values
           \item np.log(1+x) and np.sqrt(1+x)
    \end{enumerate}
    

      \item Feature generations is powered by: 
        \begin{enumerate}
           \item Prior knowledge 
           \item Exploratory data analysis
    \end{enumerate}
    
  \end{enumerate}
%
\subsubsection{Categorical and ordinal features}
%%
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
%%%
  \item \textbf{Ordinal}
    \begin{enumerate}
      \item Ordered categorical (Ticket class, education level, and driver's license etc)
      \item Ordinal vs Numerical: The differences between numerical classes are the same. It's not the case for Ordinal classes, e.g. Ticket Class    
      \item Label encoding: maps categories to numbers (For tree-based models)
        \\\indent Alphabetical (sorted: sklearn.preprocessing.LabelEncoder)
        \\\indent Order of appearance: pandas.factorize
        
      \item Frequency encoding: maps categories to their frequencies (For tree-based models usually)
        \\\indent pandas.factorize     
    \end{enumerate}
%%% 
%%% 
  \item \textbf{Categorical}
    \begin{enumerate}
      \item One-hot encoding (For non-tree based models)
                   %\\\indent pandas.get_dummies%
        \\\indent sklearn.preprocessing.OneHotEncoder
        \\\indent Allows non-tree based models (KNN, linear models) to take categorical features and improve models        
    
      \item 
    \end{enumerate}
%%%    
  \end{enumerate}
%%

%
\subsubsection{Date and coordinates}
%%
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
%%%
  \item \textbf{Date and Time}
    \begin{enumerate}
      \item Periodicity 
        \\\indent Capture repetitive patterns
      \item  Time since a particular event
        \\\indent Row-independent moment (e.g. Date stamp)
        \\\indent Row-dependent important moment (e.g. Number of days left until next holidays)       
      \item Difference between dates
        \\\indent 
    \end{enumerate}
%%% 
%%% 
  \item \textbf{Coordinates}
    \begin{enumerate}
      \item Known Coordinates               
        \\\indent Rental Prices
        \\\indent Rotate coordinates may improve models
  
      \item Unkown Coordinates
        \\\indent Extracting interesting places from train/test data or additional data
        \\\indent Canters of clusters            
         \\\indent Adding aggregated statistics (e.g. Divide the map by squares, find the most expensive flat and calculate distance of the nearest shops etc)
        
    \end{enumerate}
%%%    
  \end{enumerate}
%%


%
\subsubsection{Missing data}
%%
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
%%%
      \item Missing values can be hidden (replaced by e.g. NaNs)
      \item  The choice of method to fill NaN depends on the situation (e.g. check histogram). Usual ways to deal with NaN is to replace them with   
        \\\indent out of range value, e.g. -999
        \\\indent mean       
        \\\indent median         
  \item  Binary feature "isnull" can be beneficial
  \item In general, filling NaNs after feature generation 
  \item XGBboost can handle NaN

%%% 
%%% 
%%%    
  \end{enumerate}
%%

\subsubsection{Further reading and resource}
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
%%%
  \item \textbf{Feature preprocessing}
    \begin{enumerate}
      \item Preprocessing in Sklearn \url{https://scikit-learn.org/stable/modules/preprocessing.html}
      \item Andrew NG about gradient descent and feature scaling \url{https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling}
      \item Feature scaling and the effect of standardisation for machine learning algorithms \url{http://sebastianraschka.com/Articles/2014_about_feature_scaling.html}
      \item {Andrew Ng’s Machine Learning Course in Python (Linear Regression)} \url{https://medium.com/@ben_lau93/andrew-ngs-machine-learning-course-in-python-linear-regression-dd04fba8e137}

    \end{enumerate}
%%% 
%%% 
  \item \textbf{Feature generation}
    \begin{enumerate}
      \item  Discover Feature Engineering, How to Engineer Features and How to Get Good at It \url{https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/}
      \item  Discussion of feature engineering on Quora \url{https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering}           

    \end{enumerate}
%%%    
  \end{enumerate}
%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{ Feature extraction from text and images}
\subsubsection{Text}
\textbf{1. Preprocession}: Lowercase, stemming, lemmarization, stopwords
\\\indent \textit{(Stemming and lemmarization}: to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.)
\\\textbf{2. Bag of Words}: 
\\\indent \textit{Huge vectors}: guarantees you clear interpretation. Each feature is turned by means of having a huge amount of features. One for each unique word. 
\\\indent \textit{Ngrams}: can be applied to include words interactions for text
\\\indent \textit{TFiDF}: can be use of post-processing 
\\
\\\textbf{3. Word2Vec}:
\\\indent \textit{Small vectors}: can be applied to include words interactions for text
\\\indent \textit{Pre-trained models}
\\\indent \textit{Resource - Something to vec } \url{https://gist.github.com/nzw0301/333afc00bd508501268fa7bf40cafe4e}
\\\indent \textit{PyText for faster NLP development} \url{https://code.fb.com/ai-research/pytext-open-source-nlp-framework/}

\subsubsection{Images}
\textit{New images} integration is effective.
\\\textit{Fine tuning} 
\textit{Augmentation} is used to increase the number of images for the purpose of fine tuning.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Software/Hardware requirements}
\paragraph{}
\textup{} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Week 1 Questions }
\renewcommand{\labelenumii}{\Alph{enumii}}
\begin{enumerate}
% Begin a question
  \item In general case, should we apply these transformations to all numeric features, when we train a non-tree-based model?
    \begin{enumerate}
      \item Yes, we should apply a chosen transformation to all numeric features
      \item No, we should apply it only to few numeric features, leaving other numeric features as they were
    \end{enumerate}
 %   End a question
 
 % Begin a question
  \item Can frequency encoding be of help for non-tree based models?
    \begin{enumerate}
      \item Yes, it can
      \item No, it can't
    \end{enumerate}
 %   End a question
 
  \end{enumerate}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Week 2}

\subsection{Exploratory data analysis (EDA) is an art}
\subsubsection{Get domain knowledge} 

\subsubsection{Check if the data is intuitive}
% sessionteamplate
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
%%%
  \item \textbf{Does the data agree with domain knowledge?}
%%% 
%%% 
  \item \textbf{Understand how the data was generated}
    \begin{enumerate}
      \item  Visualisation
      \item Generate a new column (e.g. Relative days = Date-min(Date))
      \item You could correct the data manually (Age more than 200 years)
      \item  You could add some rows in the train data to make train set similar to the test set. (e.g. \url{https://jljdbvgniaoatlyhacdqym.coursera-apps.org/notebooks/readonly/reading_materials/EDA_video2.ipynb}
      
    \end{enumerate}
%%%    
  \end{enumerate}
%%

\subsubsection{Exploring anonymized data} 

    \begin{enumerate}
      \item  Data which are sensitive and changed by organisers (on purpose) 
      \item  Data which looks like hash value or with no meaningful column names
    \end{enumerate}
%%%    
%%

%%% 
\subsubsection{Explore individual features}
    \begin{enumerate}
      \item  Try to decode the column meanings by checking value\_counts and histogram distribution e.g. values could be shifted by adding a number
      \item  Guess the types of the columns 
          \begin{enumerate}
           \item df.dtypes()
            \item df.info()
             \item x.value\_counts()
              \item x.isnull()
    \end{enumerate}
  \end{enumerate}
     
%%%    
    

%%

%
\subsubsection{Feature Visualisation} 

% sessionteamplate
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
%%%
  \item \textbf{Check value distribution}
    \begin{enumerate}
      \item Histogram: plt.hist(x)
       \begin{enumerate}
        \item  Can be misleading (Can compare histograms using values or logarithm of the values)
         \item  Accumulate values in bins 
         \item If you see a high peak in the histogram (e.g. means). It could be the organisers filled missing values using mean values. How can we use this information? We can replace the missing values we found with not numbers, nulls again. For example, xgboost has a special algorithm that can fill missing values on its own and so, maybe xgboost will benefit from explicit missing values. 
      \end{enumerate}
      \item  Index vs Value plot
      Use row index as x axis, feature value as y axis plt.plot(x,'.'). This can help understand whether data is shuffled and whether there are a lot of repetitive values. 

      \item Scatter plot: Color code y values, check whether the column is a class feature; plt.scatter(range(len(x)),x, c=y) 
       \item  Statistics
      
           
    \end{enumerate}
%%% 
%%% 
  \item \textbf{Don't make a hypothesis based on a single plot}
%%%    
  \end{enumerate}
%%

%%% 


\subsubsection{Explore feature relations}
    \begin{enumerate}
      \item  \textbf{Find relations between pairs.}
        \begin{enumerate}
          \item Scatter plot
            \indent plt.scatter(x1,x2). If the a part of the test data is overlapped with training data, this is bad.
           \item Corr plot 
         \end{enumerate}
          
          \item  \textbf{Find feature groups. }
                  \begin{enumerate}
          \item Corr plot + clustering 
          \indent Diagonal (x1+x2=1). For tree based model, we could generate new feature, x1/x2 ratio or x1 and x2 difference
          \indent pd.scatter\_matrix(df)
          \indent  df.corr(), plt.matshow()
          \item Plot(Index vs feature statistics) 
            \indent  df.mean().sort\_values().plot(style='')          
         \end{enumerate}

          
    \end{enumerate}
%%%

\subsubsection{Dataset cleaning and other things to check}
    \begin{enumerate}
      \item  \textbf{Dataset cleaning}
        \begin{enumerate}
          \item \textbf{Constant features}
            \indent The train and test data set could be just a fraction of original data (e.g. only the data in 2018 was selected) 
            \\\indent If \textbf{traintest}.nunique(axis=1) ==1, then the feature could be deleted from the training set
            \\\indent  If \textbf{train}.nunique(axis=1) ==1, the feature should also be deleted from the training set
                       

           \item \textbf{Duplicated features}
            \indent  one of the duplicated columns with different column names should be removed 
            \indent  traintest.T.drop\_duplicates()
             \\\indent  The feature are identical, but feature level are different 
             \\\indent  Use \textbf{label encoding} on the two features from the \textbf{top to the bottom} row (the 1st appeared is 0, 2nd is 2) 
             \\\indent  i. for f in categorical\_features:  traintest[f]=traintest[f].factorize()    
                \\\indent  ii. traintest.T.drop\_duplicates()       


           
         \end{enumerate}
          
          \item  \textbf{Other things to check}
                  \begin{enumerate}
          \item Duplicated rows in training set could be removed
          \item Check if data is shuffled    
         \end{enumerate}

          
    \end{enumerate}


\subsubsection{Further reading and resource: Visualization tools}
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
%%%
  \item \textbf{Seaborn} \url{https://seaborn.pydata.org/}
 
  \item \textbf{Plotly} \url{https://plot.ly/python/}
  
   \item \textbf{Bokeh} \url{https://github.com/bokeh/bokeh}
 
  \item \textbf{ggplot} \url{http://ggplot.yhathq.com/}
   
  \item \textbf{Graph visualization with NetworkX} \url{https://networkx.github.io/}
  
  \item \textbf{Biclustering algorithms for sorting corrplots}   \url{http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html}
  
  %%    
  \end{enumerate}
%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Validation}
\subsubsection{Validation and overfitting}
\begin{enumerate}
%%%
  \item  \textbf{Why do we need validation? }
    \begin{enumerate}
        \item Validation helps us to evaluate the quality of the model
        \item Validation helps us to select the model which will perform best on unseen data
     \end{enumerate}
   \item  \textbf{What is underfitting? }
    \begin{enumerate}
        \item Underfitting refers to not capturing enough patterns in the data
     \end{enumerate}
     
   \item  \textbf{What is overfitting? }
    \begin{enumerate}
        \item Overfitting refers to capturing noise
        \item Capturing patterns which do not generalise to test data
        \item In competition, overfitting refers to low model's quality on test data, which was unexpected due to validation scores
     \end{enumerate}
 
  %%    
  \end{enumerate}
%%

\subsubsection{Validation strategies}

\textbf{Main validation strategies (schemes): holdout, K-Fold, LOO}

The main rule you should know — never use data you train on to measure the quality of your model. The trick is to split all your data into training and validation parts.

Below you will find several ways to validate a model.
\begin{enumerate}
   \item  \textbf{ Holdout scheme:}

Split train data into two parts: partA and partB.
Fit the model on partA, predict for partB.
Use predictions for partB for estimating model quality. Find such hyper-parameters, that quality on partB is maximized.

   \item  \textbf{ K-Fold scheme:}

Split train data into K folds.
Iterate though each fold: retrain the model on all folds except current fold, predict for the current fold.
Use the predictions to calculate quality on each fold. Find such hyper-parameters, that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement.

   \item  \textbf{ LOO (Leave-One-Out) scheme:}

Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset).
In the end you will get LOO predictions for every sample in the trainset and can calculate loss.

  \end{enumerate}
Notice, that these are validation schemes are supposed to be used to estimate quality of the model. When you found the right hyper-parameters and want to get test predictions don't forget to retrain your model using all training data.


\noindent
\subsubsection{Common validation problems}
\begin{enumerate}
   \item If we have big dispersion of scores on validation stage,
we should do extensive validation
Below you will find several ways to validate a model.
    \begin{enumerate}
      \item Average scores from different KFold splits
      \item Tune model on one split, evaluate score on the other
     \end{enumerate}
   \item If submission’s score do not match local validation score,
we should
    \begin{enumerate}
      \item Check if we have too little data in public LB
      \item Check if we overfitted
      \item Check if we chose correct splitting strategy
      \item Check if train/test have different distibutions
     \end{enumerate}
   \item Expect LB shuffle because of
       \begin{enumerate}
      \item Randomness
      \item Little amount of data
      \item Different public/private distributions
       \end{enumerate}
   \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Leakage}
\textup{How can data be leaked? }
%\textup{It's all about consistent categories. Target in labels have the same distribution For example taking the train&test as a whole category, rebalance the train data distribution}
\textup{Peculiar examples}



\subsection{Week 2 Questions }
\renewcommand{\labelenumii}{\Alph{enumii}}
\begin{enumerate}
% Begin a question
  \item Suppose we are given a huge dataset. We did a KFold validation once and noticed that scores on each fold are roughly the same. Which validation type is most practical to use?
    \begin{enumerate}
      \item We can use a simple holdout validation scheme because the data is homogeneous.
      \item We should keep on using KFold scheme as the data is homogeneous and KFold is the most computationally efficient scheme.
      \item Leave-one-out because the data is not homogeneous.
    \end{enumerate}
 %   End a question
 
 % Begin a question
  \item Suppose we are given a medium-sized dataset and we did a KFold validation once. We noticed that scores on each fold differ noticeably. Which validation type is the most practical to use?
    \begin{enumerate}
      \item Holdout
      \item LOO
      \item KFold
    \end{enumerate}

 % Begin a question
  \item The features we generate depend on the train-test data splitting method. Is this true?
    \begin{enumerate}
      \item Tue
      \item False
    \end{enumerate}
        

 %   End a question
  % Begin a question
  \item What of these can indicate an expected leaderboard shuffle in a competition?    \begin{enumerate}
      \item Most of the competitors have very similar scores
      \item Little amount of training or/and testing data
      \item Different public/private data or target distributions
    \end{enumerate}
    
  \end{enumerate}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Week 3}
\subsection{Metrics optimization}
\subsubsection{Motivation}
\subsubsection{Regression metrics}
%%
\renewcommand{\labelenumii}{\alph{enumii}}
\begin{enumerate}
\item \textbf{Regression}
\begin{enumerate}
%%%
  \item \textbf{MSE: Mean Square Error}
  \item \textbf{RMSE: Squared Mean Square Error}
  \item \textbf{RMSE: Squared Mean Square Error}
  \item \textbf{MSE vs MAE}
    \begin{enumerate}
      \item  Do you have outliers in the data?  Use MAE
      \item  Are you sure they are outliers? Use MAE
      \item   Or they are just unexpected values we should still care about? Use MSE
    \end{enumerate}
      \end{enumerate}
      \item \textbf{Discussed the metrics, sensitive to relative errors:}
      \begin{enumerate}
  \item \textbf{MSPE} Weighted version of MSE; Root MSPE
  \item \textbf{MAPE} Weighted version of MAE
  \item \textbf{MSLE} Log scaled version of MSE, Root MSLE
     \end{enumerate} 
    
%%% 
%%% 
       
 \end{enumerate}

 
%%%   
\subsection{Advanced features I}
 \begin{enumerate}
\item \textbf{Mean encoding} 
\item \textbf{Regularization}
      \begin{enumerate}
      \item CV loop inside training data
       \begin{enumerate}
          \item Robust and intuitive
          \item Usually decent results with 4-5 folds across different datasets
          \item Perfect feature for LOO scheme
          \item Target variable leakage is still present even for KFold scheme
                    \begin{figure}
    [!htb]\centering
    \includegraphics[width=5in]{w3_p3.png}
    \caption{Regiarization.CV loop}
  \label{fig:phase}
  \end{figure}
         %% \graphicspath{{/Users/j/Dropbox/kaggle/Cousera_Kaggle/}{D:\ w3_p3.png}} %% not working
          \end{enumerate}
      \item Smoothing
       \begin{enumerate}
          \item Alpha controls the amount of regularization
          \item Only works together with some other regularization method
        \end{enumerate}          
      \item Adding random noise
       \begin{enumerate}
          \item Noise degrades the quality of encoding
          \item How much noise should we add?
          \item  Usually used together with LOO
        \end{enumerate}          
      \item Sorting and calculating expanding mean        
       \begin{enumerate}
          \item Least amount of leakage
          \item No hyper parameters
          \item Irregular encoding quality
          \item Built - in in CatBoost 
        \end{enumerate}   
     \end{enumerate}
\item \textbf{Extensions and generalizations}

 \end{enumerate}
%%% 
\subsection{Metrics optimization}       


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Week 4}
\subsection{Hyperparameter tuning}
\subsection{Advanced features II}
\subsubsection{Matrix factorization}
\textbf{Matrix factorization}: MF, or matrix decomposition. MF is a way of reducing a matrix into its constituent parts. It can be applied for transforming categorical features
into real-valued.

\begin{enumerate}
%%%
  \item  \textbf{Standard MF}
    \begin{enumerate}
        \item SVD
        \item PCA 
        \\X\_all = np.concatenate \big(\big[X\_train, X\_test\big]\big)
        \\pca.fit \big(X\_all\big)
        \\X\_train\_pca = pca.transform\big(X\_train\big)
       \\X\_test\_pca = pca.transform\big(X\_test\big)
     \end{enumerate}
   \item  \textbf{TruncatedSVD}
    \begin{enumerate}
        \item For sparse matrices
        \item \url{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html}
     \end{enumerate}
     
   \item  \textbf{Non-negative Matrix Factorization (NMF)}
    \begin{enumerate}
        \item Ensures that all latent factors are non-negative
        \item Good for counts-like data
     \end{enumerate}
 
  %%    
  \end{enumerate}
%%

\subsubsection{Feature Interaction}
\begin{enumerate}
        \item \textbf{Frequent operations for feature interaction}
%%%
    \begin{enumerate}
        \item Multiplication
        \item Sum
        \item Diff
        \item Division
     \end{enumerate}
   \item  \textbf{Extract high-order interactions from decision trees}
    \begin{enumerate}
        \item Map each leaf into a binary feature \big(yes or no\big)
        \item sklearn: tree\_model.apply\big(\big)
        \item xgboost: booster.predict\big(pred\_leaf=True\big)
        
      \end{enumerate}  
 
  %%    
  \end{enumerate}
%%


\subsubsection{t-SNE}
\textbf{t-SNE} :Project points from high-dimensional space into small dimensional space, so the distances between points are approximately preserved 
\begin{enumerate}

    \item \textbf{Try different perplexities} 5-100
    \item  \textbf{How to use t-SNE effectively?}
   \\Martin Wattenberg, \url{https://distill.pub/2016/misread-tsne/}
     \item \textbf{Tools}
         \begin{enumerate}
        \item tsne 0.1.8 python (fast)
        \item t-SNE R (single-cell)
         \item sklearn :sklearn.manifold.TSNE
          \end{enumerate}
       \item \textbf{Note}
     \begin{enumerate}
        \item tSNE is a great tool for visualization
        \item It can be used as feature as well
        \item Be careful with interpretation of results
        \item Try different perplexitie
       \end{enumerate}

 
  %%    
  \end{enumerate}
%%

\pagebreak
\section{Week 5}
\subsection{Kaggle past solutions}
\begin{enumerate}
    \item \url{http://ndres.me/kaggle-past-solutions}
    %%\item \url{https://www.kaggle.com/wiki/PastSolutions}
    \item \url{http://www.chioka.in/kaggle-competition-solutions}
    \item \url{https://github.com/ShuaiW/kaggle-classification}
 \end{enumerate}    

\subsection{A lot of cheetsheets}
\url{https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463}

\end{document}  